\babel@toc {brazil}{}
\contentsline {chapter}{\numberline {1}Introduction}{15}{chapter.1}
\contentsline {section}{\numberline {1.1}Motivation}{15}{section.1.1}
\contentsline {subsection}{\numberline {1.1.1}AI Super Powers}{15}{subsection.1.1.1}
\contentsline {subsection}{\numberline {1.1.2}Deep Learning Revolution}{18}{subsection.1.1.2}
\contentsline {subsection}{\numberline {1.1.3}Reinforcement Learning}{19}{subsection.1.1.3}
\contentsline {section}{\numberline {1.2}Contextualization}{19}{section.1.2}
\contentsline {subsection}{\numberline {1.2.1}Simulated Soccer 2D}{20}{subsection.1.2.1}
\contentsline {section}{\numberline {1.3}Objective}{20}{section.1.3}
\contentsline {section}{\numberline {1.4}Scope}{20}{section.1.4}
\contentsline {section}{\numberline {1.5}Organization of this work}{21}{section.1.5}
\contentsline {chapter}{\numberline {2}Soccer 2D}{24}{chapter.2}
\contentsline {section}{\numberline {2.1}How the Simulator Works}{24}{section.2.1}
\contentsline {section}{\numberline {2.2}Physical model of rcssserver}{25}{section.2.2}
\contentsline {subsection}{\numberline {2.2.1}Movement of the object}{25}{subsection.2.2.1}
\contentsline {section}{\numberline {2.3}Player Model}{25}{section.2.3}
\contentsline {subsection}{\numberline {2.3.1}Available Action Commands}{25}{subsection.2.3.1}
\contentsline {section}{\numberline {2.4}The Penalty Event}{26}{section.2.4}
\contentsline {chapter}{\numberline {3}Deep Reinforcement Learning}{27}{chapter.3}
\contentsline {section}{\numberline {3.1}Markov Decision Processes}{27}{section.3.1}
\contentsline {subsection}{\numberline {3.1.1}Important Concepts}{28}{subsection.3.1.1}
\contentsline {subsubsection}{\numberline {3.1.1.1}Reward Function}{28}{subsubsection.3.1.1.1}
\contentsline {subsubsection}{\numberline {3.1.1.2}Transition Function}{28}{subsubsection.3.1.1.2}
\contentsline {subsubsection}{\numberline {3.1.1.3}Discounting}{28}{subsubsection.3.1.1.3}
\contentsline {subsubsection}{\numberline {3.1.1.4}Utility}{29}{subsubsection.3.1.1.4}
\contentsline {subsubsection}{\numberline {3.1.1.5}Policy}{29}{subsubsection.3.1.1.5}
\contentsline {subsubsection}{\numberline {3.1.1.6}Value Function}{30}{subsubsection.3.1.1.6}
\contentsline {subsubsection}{\numberline {3.1.1.7}Q-State}{30}{subsubsection.3.1.1.7}
\contentsline {subsubsection}{\numberline {3.1.1.8}Action Value Function}{30}{subsubsection.3.1.1.8}
\contentsline {subsection}{\numberline {3.1.2}The Bellman Equations}{30}{subsection.3.1.2}
\contentsline {subsubsection}{\numberline {3.1.2.1}Optimal Quantities}{30}{subsubsection.3.1.2.1}
\contentsline {subsubsection}{\numberline {3.1.2.2}The Equations}{30}{subsubsection.3.1.2.2}
\contentsline {subsection}{\numberline {3.1.3}About Solving It}{31}{subsection.3.1.3}
\contentsline {subsection}{\numberline {3.1.4}The Problem With The Tabular Solution}{31}{subsection.3.1.4}
\contentsline {section}{\numberline {3.2}Neural Networks}{32}{section.3.2}
\contentsline {subsection}{\numberline {3.2.1}Neurons}{32}{subsection.3.2.1}
\contentsline {subsection}{\numberline {3.2.2}The Network}{33}{subsection.3.2.2}
\contentsline {subsection}{\numberline {3.2.3}Cost Function and Gradient Descent}{33}{subsection.3.2.3}
\contentsline {subsection}{\numberline {3.2.4}Back Propagation}{34}{subsection.3.2.4}
\contentsline {subsubsection}{\numberline {3.2.4.1}Notation}{34}{subsubsection.3.2.4.1}
\contentsline {subsubsection}{\numberline {3.2.4.2}Neuron Error Definition}{34}{subsubsection.3.2.4.2}
\contentsline {subsubsection}{\numberline {3.2.4.3}Back Propagation Equations}{34}{subsubsection.3.2.4.3}
\contentsline {subsubsection}{\numberline {3.2.4.4}Neural Networks in Reinforcement Learning}{35}{subsubsection.3.2.4.4}
\contentsline {section}{\numberline {3.3}Policy Gradient Methods, PPO}{35}{section.3.3}
\contentsline {subsection}{\numberline {3.3.1}Notation and Concepts}{36}{subsection.3.3.1}
\contentsline {subsubsection}{\numberline {3.3.1.1}The parameters $\bm {\theta }$}{36}{subsubsection.3.3.1.1}
\contentsline {subsubsection}{\numberline {3.3.1.2}The Score Function}{36}{subsubsection.3.3.1.2}
\contentsline {subsection}{\numberline {3.3.2}Policy Approximation and its Advantages}{36}{subsection.3.3.2}
\contentsline {subsection}{\numberline {3.3.3}Policy Gradient Vanilla}{37}{subsection.3.3.3}
\contentsline {subsubsection}{\numberline {3.3.3.1}Issues with the Vanilla Version}{37}{subsubsection.3.3.3.1}
\contentsline {subsection}{\numberline {3.3.4}Actor Critic}{37}{subsection.3.3.4}
\contentsline {subsection}{\numberline {3.3.5}Proximal Policy Optimization (PPO)}{38}{subsection.3.3.5}
\contentsline {subsubsection}{\numberline {3.3.5.1}Replacing the log}{38}{subsubsection.3.3.5.1}
\contentsline {subsubsection}{\numberline {3.3.5.2}The Clipped Surrogate Objective}{39}{subsubsection.3.3.5.2}
\contentsline {subsubsection}{\numberline {3.3.5.3}The Final Score Function}{39}{subsubsection.3.3.5.3}
\contentsline {subsubsection}{\numberline {3.3.5.4}Multiple epochs for policy updating}{39}{subsubsection.3.3.5.4}
\contentsline {chapter}{\numberline {4}To Do List}{41}{chapter.4}
\contentsline {chapter}{Refer\^encias}{42}{chapter.5}
